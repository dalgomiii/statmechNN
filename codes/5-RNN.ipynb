{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import sympy as sy\n",
    "import random as rd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the model\n",
    "We will use a restricted Boltzmann machine with $M$ visible nodes and 1 hidden node. We represent the weights with an adjacency matrix and the node values with a feature vector. The restricted Boltzmann machine is a bipartite graph, thus we can easily construct the adjacency matrix with the feature vectors when we order with visible nodes first, then hidden nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model generating true feature vector / set of weights / set of coupling constants\n",
    "#Generate no_feature_vec models with different feature vectors randomly\n",
    "def Generate_teacher_models(no_teacher_models, no_visible_nodes:int, seed:int=100 , no_hidden_nodes:int= 1, binary:bool=True):\n",
    "    rd.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    teacher_models_adj_mat=[]\n",
    "    teacher_models_feature_mat=[]\n",
    "    for i in range(no_teacher_models):\n",
    "        if binary:\n",
    "            # for binary weights\n",
    "            feature_mat=np.random.choice([-1,1], size=(no_visible_nodes, no_hidden_nodes))\n",
    "        else:\n",
    "            #for cotinuous weights from -1 to 1 with uniform probability\n",
    "            feature_mat=np.random.uniform(-1,1, size=(no_visible_nodes, no_hidden_nodes))\n",
    "\n",
    "        adj_mat=np.zeros((no_visible_nodes+no_hidden_nodes,no_visible_nodes+no_hidden_nodes))\n",
    "        adj_mat[0:no_visible_nodes,no_visible_nodes:]=feature_mat\n",
    "        half_mat=adj_mat.copy()\n",
    "        adj_mat=half_mat+adj_mat.T\n",
    "\n",
    "        teacher_models_adj_mat.append(adj_mat)\n",
    "        teacher_models_feature_mat.append(feature_mat)\n",
    "\n",
    "    \n",
    "    return teacher_models_adj_mat, teacher_models_feature_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising Parameters\n",
    "Initialising the number of teacher model and the number of hidden nodes to create the feature matrix(vector) and the adjaecency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 1 #Number of teacher model\n",
    "M = 60 #Number of hidden node\n",
    "teacher_model_adj_mat,teacher_model_feature_mat=Generate_teacher_models(no, M,seed=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"This is the feature matirx:\\n\", teacher_model_feature_mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the adjacency matrix:\\n\",np.array(teacher_model_adj_mat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the functions for Metropolis Algorithm for the Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamiltonian(nodes, adj_mat):\n",
    "    if len(nodes)!=len(adj_mat):\n",
    "        print(\"NOOOOO! number of nodes not eq to size of adj_mat!!!\")\n",
    "        print(adj_mat)\n",
    "        print(nodes)\n",
    "    H=-nodes@adj_mat@nodes.T\n",
    "    return H\n",
    "\n",
    "def transition_probability(nodes,new_nodes,adj_mat,inv_temperature):\n",
    "    adj_mat=adj_mat.copy()\n",
    "    H_new=Hamiltonian(nodes=new_nodes,adj_mat=adj_mat)\n",
    "    H=Hamiltonian(nodes=nodes,adj_mat=adj_mat)\n",
    "    diff_H=H_new-H\n",
    "    if diff_H < 0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(-inv_temperature*diff_H)\n",
    "\n",
    "def concat_sublist(mylist):\n",
    "    return [item for sublist in mylist for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis Algorithm\n",
    "1. Initialise the Adjacency Matrix for nodes in state $s$\n",
    "2. Flip a random node to determined the next step state $s'$\n",
    "3. Calculate the energy change when the node is flipped\n",
    "4. If the energy change,$\\Delta E \\leq 0$, update state $s=s'$\n",
    "5. Else, flip the spin only if $\\exp{(\\Delta E/k_bT)} \\geq r$ where $r$ is a random number such that $0<r<1$\n",
    "6. Repeat steps 1 to 5, until equilibrium has reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Equilibrium and Sampling\n",
    "Equilibrium is determined by checking the correlation coefficient within a moving window. To ensure the each collected sample are uncorrelated, we will identify how many Monte Carlo sweeps will be sufficient, $\\delta t$. Then, collect samples every $\\delta t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metropolis algo\n",
    "def metropolis(no_sample, adj_mat, inv_temperature, window_size, no_of_nodes, ensemble_size, window_r_limit, seed:int=102, printing:bool=False):\n",
    "    adj_mat=adj_mat.copy()\n",
    "    eqm=0 #becomes 1 when eqm is reached\n",
    "    counter=0 #time\n",
    "    samples=[] #samples to be collected\n",
    "    delta_t_found=0\n",
    "\n",
    "    #generate initial set of nodes/ data vector/ set of neuron activations\n",
    "    rd.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    initial_nodes=np.random.choice([-1,1], size=no_of_nodes)\n",
    "    \n",
    "    #create an ensemble of data vectors\n",
    "    nodes_ensemble=[initial_nodes.copy() for i in range(ensemble_size)]\n",
    "    \n",
    "    #calculate the Hamiltonians for each data vector in the ensemble at time=0\n",
    "    nodes=initial_nodes.copy()\n",
    "    H_timeseries_ensemble=[]\n",
    "    H_timeseries_ensemble.append([Hamiltonian(nodes=nodes_ensemble[i],adj_mat=adj_mat) for i in range(ensemble_size)])\n",
    "    \n",
    "    while len(samples) < no_sample:\n",
    "        for i in range(ensemble_size):\n",
    "            nodes=nodes_ensemble.copy()[i]\n",
    "\n",
    "            #create flipped node\n",
    "            flipped_nodes=nodes.copy()\n",
    "            random_index = np.random.randint(0, len(flipped_nodes))\n",
    "            flipped_nodes[random_index]*=(-1)\n",
    "\n",
    "            #determining transition\n",
    "            if transition_probability(nodes=nodes,new_nodes=flipped_nodes,adj_mat=adj_mat,inv_temperature=inv_temperature) > np.random.rand():\n",
    "                nodes=flipped_nodes #updating nodes to the flipped nodes\n",
    "            \n",
    "            #update the ensemble of set of nodes\n",
    "            nodes_ensemble[i]=nodes\n",
    "\n",
    "        #calculate the Hamiltonians for each data vector in the ensemble at time>0\n",
    "        H_timeseries_ensemble.append([Hamiltonian(nodes=nodes_ensemble[i],adj_mat=adj_mat) for i in range(ensemble_size)])\n",
    "            \n",
    "        #determine whether equilibrium is reached by checking that the Pearson correlation coefficient is less than a defined value\n",
    "        if counter>window_size and eqm==0:\n",
    "            window=concat_sublist(H_timeseries_ensemble[-window_size:])\n",
    "            x=concat_sublist([[i]*ensemble_size for i in range(window_size)])\n",
    "            window_r=np.corrcoef(x,window)[0,1]\n",
    "            if abs(window_r)<window_r_limit:\n",
    "                eqm=1\n",
    "                if printing:\n",
    "                    print(\"Eqm reached at t=\",counter)\n",
    "                t_eqm=counter-window_size\n",
    "            if np.std(np.array(window),ddof=1)==0.0:\n",
    "                eqm=1\n",
    "                if printing:\n",
    "                    print(\"Eqm reached at t=\",counter)\n",
    "                t_eqm=counter-window_size\n",
    "        \n",
    "                \n",
    "        #find time interval to collect samples to ensure that the samples are not correlated\n",
    "        if eqm==1 and delta_t_found==0:\n",
    "            time_r=np.corrcoef(H_timeseries_ensemble[counter],H_timeseries_ensemble[t_eqm])[0,1]\n",
    "            if time_r < 1/np.e or np.std(np.array(H_timeseries_ensemble[counter]))==0 or np.std(np.array(H_timeseries_ensemble[t_eqm]))==0:\n",
    "                delta_t=counter-t_eqm\n",
    "                if printing:\n",
    "                    print(\"delta_t found to be\", delta_t)\n",
    "                delta_t_found=1\n",
    "\n",
    "        #collect samples\n",
    "        if eqm==1 and delta_t_found==1:\n",
    "            if counter%delta_t==0:\n",
    "                samples.append(nodes_ensemble[0])\n",
    "                if printing:\n",
    "                    print(\"Added\")\n",
    "                if len(samples)==1:\n",
    "                    t_first_sample=counter\n",
    "                if len(samples)==no_sample:\n",
    "                    t_last_sample=counter\n",
    "\n",
    "        counter+=1\n",
    "\n",
    "        if counter%no_of_nodes==0 and printing:\n",
    "            print(\"MCS=\",counter//no_of_nodes)\n",
    "    \n",
    "    return samples, t_eqm, delta_t, t_first_sample, t_last_sample, H_timeseries_ensemble\n",
    "\n",
    "#t1 and t2(inculisve) defines the window size we want to plot, Y is an ensemble of time series data (i.e. a list of lists)\n",
    "def plotting_random_walks_ensemble_of_timeseries(t1,t2,Y):\n",
    "    x=np.arange(t1,t2+1)\n",
    "    for times_series in Y:\n",
    "        y = times_series[t1:t2+1]  # Random data for demonstration\n",
    "        # Create the plot\n",
    "        plt.plot(x, y)\n",
    "    plt.ylabel(\"Hamiltonian\")\n",
    "    plt.xlabel(\"Monte Carlo Sweeps (MCS)\")\n",
    "    plt.title(\"Plot of Hamiltonian against MCS at ß=%1.2f\" % beta )\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "#t1 and t2(inculisve) defines the window size we want to plot, Y is a time series of an ensemble of data (i.e. a list of lists)\n",
    "def plotting_random_walks_timeseries_of_ensemble(t1,t2,Y):\n",
    "    x=np.arange(t1,t2+1)\n",
    "    Y=np.array(Y)\n",
    "    for i in range(len(Y[0,:])):\n",
    "        y = Y[t1:t2+1,i] \n",
    "        plt.plot(x, y)\n",
    "    plt.ylabel(\"Hamiltonian\")\n",
    "    plt.xlabel(\"Monte Carlo Sweeps (MCS)\")\n",
    "    plt.title(\"Plot of Hamiltonian against MCS at ß=%1.2f\" % beta)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Metropolis Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many samples to collect\n",
    "no_of_samples = 60\n",
    "#inverse temperature\n",
    "beta = 0.5\n",
    "#determining how large the window size will be \n",
    "window_size = 20\n",
    "#number of random walk to run simultaneously\n",
    "no_of_random_walks = 30\n",
    "#upper bound of correlation coefficient within window\n",
    "corcoeff=0.1\n",
    "#seed of simulation for repetition (optional)\n",
    "seed = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, t_eqm, delta_t, t_first_sample, t_last_sample, H_timeseries_ensemble=metropolis(no_sample=no_of_samples, adj_mat=teacher_model_adj_mat[0], inv_temperature=beta, window_size=window_size, no_of_nodes=M+1, ensemble_size=no_of_random_walks, window_r_limit=corcoeff, seed=seed, printing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the entire random walk of the Monte Carlo Simlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_random_walks_timeseries_of_ensemble(t1=0,t2=t_last_sample,Y=H_timeseries_ensemble)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the random walk of the Monte Carlo Simulation when Equilibrium has reached (when each walk have low correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_random_walks_timeseries_of_ensemble(t1=t_first_sample,t2=t_last_sample,Y=H_timeseries_ensemble)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing for Inverse Ising problem\n",
    "\n",
    "To reconstruct the feature vector we need to do message passing using these equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_{i\\rightarrow a}=\\tanh{\\left( \\sum_{b \\in \\partial i \\backslash \\{a\\}} u_{b \\rightarrow i} \\right)}$\n",
    "\n",
    "$u_{b \\rightarrow i}=\\tanh^{-1}{\\left( \\tanh{\\left( \\beta G_{b \\rightarrow i} \\right)} \\tanh{\\left( \\beta \\sigma_{i}^b / \\sqrt{N} \\right)} \\right)} $\n",
    "\n",
    "$G_{b \\rightarrow i} = \\frac{1}{\\sqrt{N}} \\sum_{j \\in \\partial b \\backslash \\{i\\}} \\sigma_{jb} m_{j \\rightarrow b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Message passing equations for inverse Ising problem\n",
    "\n",
    "def m_i_a_new_inv(u_a_i_old):\n",
    "    selection_list=[np.array([[1 if i != j else 0 for j in range(len(u_a_i_old))]]) for i in range(len(u_a_i_old))]\n",
    "    m_i_a_new=np.tanh(np.concatenate([selection_list[i]@u_a_i_old for i in range(len(selection_list))],axis=0)).T\n",
    "    return m_i_a_new\n",
    "\n",
    "def G_a_i_inv(m_i_a,sigma_ia):\n",
    "    selection_list=[np.array([[1 if i != j else 0 for j in range(len(m_i_a))]]) for i in range(len(m_i_a))]\n",
    "    G_a_i=(1/np.sqrt(len(sigma_ia)))*np.concatenate([selection_list[i]@(m_i_a*sigma_ia) for i in range(len(selection_list))],axis=0).T\n",
    "    return G_a_i\n",
    "\n",
    "def u_a_i_new_inv(G_a_i_old,sigma_ia,inv_temperature):\n",
    "    return np.arctanh(np.tanh(inv_temperature*G_a_i_old)*np.tanh(inv_temperature*sigma_ia.T/np.sqrt(len(sigma_ia))))\n",
    "\n",
    "#Message passing iteration for inverse Ising problem:\n",
    "#i is the index for visible nodes, a is the index for which sample\n",
    "def message_passing_iteration_inv(sigma_ia,convergence_size,inv_temperature,seed,printing:bool,binary=True):\n",
    "    sigma_ia=sigma_ia.copy()\n",
    "    no_i=len(sigma_ia)\n",
    "    no_a=len(sigma_ia[0,:])\n",
    "\n",
    "    #initialize m_i_a and u_a_i\n",
    "    rd.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if binary:\n",
    "        m_i_a_0 = np.random.choice([-1,1], size=(no_i, no_a))\n",
    "    else:\n",
    "        m_i_a_0 = np.random.uniform(-1,1, size=(no_i, no_a))\n",
    "    G_a_i_0=G_a_i_inv(m_i_a_0,sigma_ia)\n",
    "    u_a_i_0=u_a_i_new_inv(G_a_i_0,sigma_ia,inv_temperature)\n",
    "\n",
    "    #define the updated m_i_a and u_a_i to enter the while loop\n",
    "    m_i_a_1 = np.ones((no_i, no_a))*10\n",
    "    u_a_i_1 = (np.ones((no_i, no_a))*10).T\n",
    "    i=0\n",
    "    while not (np.all(np.abs(m_i_a_1-m_i_a_0) <= convergence_size) and  np.all(np.abs(u_a_i_1-u_a_i_0) <= convergence_size)):\n",
    "        if i!=0:\n",
    "            m_i_a_0=m_i_a_1\n",
    "            u_a_i_0=u_a_i_1\n",
    "        \n",
    "        #calculate the updated m_i_a and u_a_i\n",
    "        m_i_a_1=m_i_a_new_inv(u_a_i_0)\n",
    "        G_a_i_1=G_a_i_inv(m_i_a_1,sigma_ia)\n",
    "        u_a_i_1=u_a_i_new_inv(G_a_i_1,sigma_ia,inv_temperature)\n",
    "\n",
    "        i+=1\n",
    "        if printing:\n",
    "            print(i, \"message passing iterations done.\")\n",
    "\n",
    "    return m_i_a_1, u_a_i_1\n",
    "\n",
    "def magnetization_from_cavity_fields(u_a_i):\n",
    "    return np.tanh(np.sum(u_a_i, axis=0))\n",
    "\n",
    "def solution_inv_Ising(m_i):\n",
    "    return np.sign(m_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_ia=np.array(sample)[:,:len(sample[0])-1].T\n",
    "\n",
    "m_i_a, u_a_i = message_passing_iteration_inv(sigma_ia,0.1,inv_temperature=2,seed=100,printing=True)\n",
    "\n",
    "result=solution_inv_Ising(magnetization_from_cavity_fields(u_a_i))\n",
    "accuracy_vector=1-np.abs((teacher_model_feature_mat[0].T)[0,:]-result)/2\n",
    "accuracy=np.sum(accuracy_vector)/len(accuracy_vector)\n",
    "print(\"Predicted Feature Vector:\\n\",result)\n",
    "print(\"Truth Vector\\n\",(teacher_model_feature_mat[0].T)[0,:])\n",
    "print(\"Accuracy: \",accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Z_2$ Symmertry\n",
    "However, due to $Z_2$ symmetry, the accuracy from the similarity between the predicted feature vector and truth vector is not an excellent indication of learning. We need to define the overlap $q=  \\left|\\overline{ J_i^{\\text{true}} \\hat{J_i}  }\\right| $ where $\\overline{\\bullet}$ is the average over all variable nodes. A $q$ near zero indicates that the student RBM is performing not much better than random guessing. The modulus appearing in the expression for $q$ emphasizes that having a very low accuracy in predicting the weights is in fact favourable. A student RBM with a perfectly wrong feature matrix will yield exactly the same output spin value statistics as a perfectly correct student RBM. This is the result of $Z_2$ symmetry present in the model with no external fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(feature_mat,predicted_feature_mat):\n",
    "    return abs((1/len(predicted_feature_mat))*np.sum(feature_mat*predicted_feature_mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_result=q(teacher_model_feature_mat[0].T,result)\n",
    "print(\"The overlap q =\",q_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try high temperature\n",
    "\n",
    "To understand how temperature affect learning, we can increase the temperature, and in turn decreasing the inverse temperature and compare the accuracy with the one at lower temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse temperature at higher temperature\n",
    "beta = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample, t_eqm, delta_t, t_first_sample, t_last_sample, H_timeseries_ensemble=metropolis(no_sample=no_of_samples, adj_mat=teacher_model_adj_mat[0], inv_temperature=beta, window_size=window_size, no_of_nodes=M+1, ensemble_size=no_of_random_walks, window_r_limit=corcoeff, seed=seed, printing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the entire random walk of the Monte Carlo Simlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_random_walks_timeseries_of_ensemble(t1=0,t2=t_last_sample,Y=H_timeseries_ensemble)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_random_walks_timeseries_of_ensemble(t1=t_first_sample,t2=t_last_sample,Y=H_timeseries_ensemble)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_ia=np.array(sample)[:,:len(sample[0])-1].T\n",
    "\n",
    "m_i_a, u_a_i = message_passing_iteration_inv(sigma_ia,0.1,inv_temperature=2,seed=100,printing=True)\n",
    "\n",
    "result=solution_inv_Ising(magnetization_from_cavity_fields(u_a_i))\n",
    "accuracy_vector=1-np.abs((teacher_model_feature_mat[0].T)[0,:]-result)/2\n",
    "accuracy=np.sum(accuracy_vector)/len(accuracy_vector)\n",
    "print(\"Predicted Feature Vector:\\n\",result)\n",
    "print(\"Truth Vector\\n\",(teacher_model_feature_mat[0].T)[0,:])\n",
    "print(\"Accuracy: \",accuracy*100,\"%\")\n",
    "q_result=q(teacher_model_feature_mat[0].T,result)\n",
    "print(\"The overlap q =\",q_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
