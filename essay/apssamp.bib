@book{yeomans1992statistical,
  title={Statistical Mechanics of Phase Transitions},
  author={Yeomans, J.M.},
  isbn={9780191589706},
  url={https://books.google.com.sg/books?id=3IUVSvOUtTMC},
  year={1992},
  publisher={Clarendon Press}
}

@article{schmidhuber2022annotated,
  title={Annotated history of modern AI and Deep learning},
  author={Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2212.11279},
  year={2022}
}

@inbook{huang2021statistical_boltzmannM_H,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={111},
  year={2021},
  publisher={Springer}
}
@inbook{huang2021statistical_msim,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={5-8},
  year={2021},
  publisher={Springer}
}
@inbook{huang2021statistical_TAP,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={43-52},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_self_avg,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={66},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_replica_trick,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={66-67},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_replica_F,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={67-72},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_replica_sym,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
pages={73},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_replica_sym_break,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={9},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_cavity,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={2},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_boltzmannM,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={10},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_simplestModel,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={11},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_Bethe,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={3.2.2},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_cavity_F,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={2.2},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_cavity_MP,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={2.3},
  year={2021},
  publisher={Springer}
}

@inbook{huang2021statistical_inv_Ising,
  booktitle={Statistical mechanics of neural networks},
  author={Huang, Haiping},  
chapter={3.3},
  year={2021},
  publisher={Springer}
}

@article{huang2016unsupervised,
  title={Unsupervised feature learning from finite data by message passing: discontinuous versus continuous phase transition},
  author={Huang, Haiping and Toyoizumi, Taro},
  journal={Physical Review E},
  volume={94},
  number={6},
  pages={062310},
  year={2016},
  publisher={APS}
}

@incollection{SCHNEIDER2022149,
title = {Chapter 8 - Machine learning: ML for eHealth systems},
editor = {Patrick Schneider and Fatos Xhafa},
booktitle = {Anomaly Detection and Complex Event Processing over IoT Data Streams},
publisher = {Academic Press},
pages = {149-191},
year = {2022},
isbn = {978-0-12-823818-9},
doi = {https://doi.org/10.1016/B978-0-12-823818-9.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128238189000195},
author = {Patrick Schneider and Fatos Xhafa},
keywords = {Algorithms, Diagnostic systems, Ethics, safety, and equity, Learning problems, Learning techniques, Ethics, safety, privacy, accountability, and transparency, ML frameworks, Federated learning}
}

@article{Castellani_2005,
doi = {10.1088/1742-5468/2005/05/P05012},
url = {https://dx.doi.org/10.1088/1742-5468/2005/05/P05012},
year = {2005},
month = {may},
publisher = {},
volume = {2005},
number = {05},
pages = {P05012},
author = {Tommaso Castellani and Andrea Cavagna},
title = {Spin-glass theory for pedestrians},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
}

@article{rumelhart1986parallel,
  title={Parallel distributed processing, volume 1: Explorations in the microstructure of cognition: Foundations},
  author={Rumelhart, David E and McClelland, James L and PDP Research Group and others},
  year={1986},
  publisher={The MIT Press}
}
@book{charu2018neural,
  title={Neural networks and deep learning: a textbook},
  author={Charu, C Aggarwal},
  year={2018},
  publisher={Spinger}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@book{murphy2012machine,
  title={Machine learning: a probabilistic perspective},
  author={Murphy, Kevin P},
  year={2012},
  publisher={MIT press}
}
@article{melko2019restricted,
  title={Restricted Boltzmann machines in quantum physics},
  author={Melko, Roger G and Carleo, Giuseppe and Carrasquilla, Juan and Cirac, J Ignacio},
  journal={Nature Physics},
  volume={15},
  number={9},
  pages={887--892},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@article{larochelle2012learning,
  title={Learning algorithms for the classification restricted Boltzmann machine},
  author={Larochelle, Hugo and Mandel, Michael and Pascanu, Razvan and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={643--669},
  year={2012},
  publisher={JMLR. org}
}
@article{
doi:10.1073/pnas.79.8.2554,
author = {J J Hopfield },
title = {Neural networks and physical systems with emergent collective computational abilities.},
journal = {Proceedings of the National Academy of Sciences},
volume = {79},
number = {8},
pages = {2554-2558},
year = {1982},
doi = {10.1073/pnas.79.8.2554},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554}
}

@article{amari1972learning,
  title={Learning patterns and pattern sequences by self-organizing nets of threshold elements},
  author={Amari, S-I},
  journal={IEEE Transactions on computers},
  volume={100},
  number={11},
  pages={1197--1206},
  year={1972},
  publisher={IEEE}
}
@article{amari1972characteristics,
  title={Characteristics of random nets of analog neuron-like elements},
  author={Amari, Shun-Ichi},
  journal={IEEE Transactions on systems, man, and cybernetics},
  number={5},
  pages={643--657},
  year={1972},
  publisher={IEEE}
}
@article{dupond2019thorough,
  title={A thorough review on the current advance of neural network structures},
  author={Dupond, Samuel},
  journal={Annual Reviews in Control},
  volume={14},
  number={14},
  pages={200--230},
  year={2019}
}

@article{tealab2018time,
  title={Time series forecasting using artificial neural networks methodologies: A systematic review},
  author={Tealab, Ahmed},
  journal={Future Computing and Informatics Journal},
  volume={3},
  number={2},
  pages={334--340},
  year={2018},
  publisher={Elsevier}
}
@article{sherrington1975solvable,
  title={Solvable model of a spin-glass},
  author={Sherrington, David and Kirkpatrick, Scott},
  journal={Physical review letters},
  volume={35},
  number={26},
  pages={1792},
  year={1975},
  publisher={APS}
}
@article{kramer1991nonlinear,
  title={Nonlinear principal component analysis using autoassociative neural networks},
  author={Kramer, Mark A},
  journal={AIChE journal},
  volume={37},
  number={2},
  pages={233--243},
  year={1991},
  publisher={Wiley Online Library}
}
@article{spall2003estimation,
  title={Estimation via markov chain monte carlo},
  author={Spall, James C},
  journal={IEEE Control Systems Magazine},
  volume={23},
  number={2},
  pages={34--45},
  year={2003},
  publisher={IEEE}
}
@inproceedings{rosenbluth2003genesis,
  title={Genesis of the Monte Carlo algorithm for statistical mechanics},
  author={Rosenbluth, Marshall N},
  booktitle={AIP Conference Proceedings},
  volume={690},
  number={1},
  pages={22--30},
  year={2003},
  organization={American Institute of Physics}
}

@article{Fedorenko_2003,
doi = {10.1088/0305-4470/36/5/304},
url = {https://dx.doi.org/10.1088/0305-4470/36/5/304},
year = {2003},
month = {jan},
publisher = {},
volume = {36},
number = {5},
pages = {1239},
author = {A A Fedorenko},
title = {Replicon modes and stability of critical behaviour of disordered systems with respect to the continuous replica symmetry breaking},
journal = {Journal of Physics A: Mathematical and General},
abstract = {A field-theory approach is used to investigate the ‘spin-glass effects’ on the critical behaviour of systems with weak temperature-like quenched disorder. The renormalization group (RG) analysis of the effective Hamiltonian of a model with replica symmetry breaking (RSB) potentials of a general type is carried out in the two-loop approximation. The fixed point (FP) stability, recently found within the one-step RSB RG treatment, is further explored in terms of replicon eigenvalues. We find that the traditional FPs, which are usually considered to describe the disorder-induced universal critical behaviour, remain stable when the continuous RSB modes are taken into account.}

@article{li2018neural,
  title={Neural network renormalization group},
  author={Li, Shuo-Hui and Wang, Lei},
  journal={Physical review letters},
  volume={121},
  number={26},
  pages={260601},
  year={2018},
  publisher={APS}
}

@article{doi:10.1146/annurev-conmatphys-031119-050745,
author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
title = {Statistical Mechanics of Deep Learning},
journal = {Annual Review of Condensed Matter Physics},
volume = {11},
number = {1},
pages = {501-528},
year = {2020},
doi = {10.1146/annurev-conmatphys-031119-050745},

URL = { 
    
        https://doi.org/10.1146/annurev-conmatphys-031119-050745
    
    

},
eprint = { 
    
        https://doi.org/10.1146/annurev-conmatphys-031119-050745
    
    

}
,
    abstract = { The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward. }
}

@article{DEBUS2021116459,
title = {Deep learning in analytical chemistry},
journal = {TrAC Trends in Analytical Chemistry},
volume = {145},
pages = {116459},
year = {2021},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2021.116459},
url = {https://www.sciencedirect.com/science/article/pii/S016599362100282X},
author = {Bruno Debus and Hadi Parastar and Peter Harrington and Dmitry Kirsanov},
keywords = {Deep learning, Chemometrics, Data analysis, Convolutional neural networks, Machine learning},
abstract = {In recent years, extensive research in the field of Deep Learning (DL) has led to the development of a wide array of machine learning algorithms dedicated to solving complex tasks such as image classification or speech recognition. Due to their unprecedented ability to explore large volumes of data and extract meaningful hidden structures, DL models have naturally drawn attention from various fields in science. Analytical chemistry, in particular, has successfully benefited from the application of DL tools for extracting qualitative and quantitative information from high-dimensional and complex chemical measurements. This report provides introductory reading for understanding DL machinery and reviews recent analytical applications of these powerful algorithms.}
}

@article{jimenez2020drug,
  title={Drug discovery with explainable artificial intelligence},
  author={Jim{\'e}nez-Luna, Jos{\'e} and Grisoni, Francesca and Schneider, Gisbert},
  journal={Nature Machine Intelligence},
  volume={2},
  number={10},
  pages={573--584},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{Shlomi_2021,
doi = {10.1088/2632-2153/abbf9a},
url = {https://dx.doi.org/10.1088/2632-2153/abbf9a},
year = {2020},
month = {dec},
publisher = {IOP Publishing},
volume = {2},
number = {2},
pages = {021001},
author = {Jonathan Shlomi and Peter Battaglia and Jean-Roch Vlimant},
title = {Graph neural networks in particle physics},
journal = {Machine Learning: Science and Technology},
abstract = {Particle physics is a branch of science aiming at discovering the fundamental laws of matter and forces. Graph neural networks are trainable functions which operate on graphs—sets of elements and their pairwise relations—and are a central method within the broader field of geometric deep learning. They are very expressive and have demonstrated superior performance to other classical deep learning approaches in a variety of domains. The data in particle physics are often represented by sets and graphs and as such, graph neural networks offer key advantages. Here we review various applications of graph neural networks in particle physics, including different graph constructions, model architectures and learning objectives, as well as key open problems in particle physics for which graph neural networks are promising.}
}

@article{george2018deep,
  title={Deep neural networks to enable real-time multimessenger astrophysics},
  author={George, Daniel and Huerta, EA},
  journal={Physical Review D},
  volume={97},
  number={4},
  pages={044039},
  year={2018},
  publisher={APS}
}